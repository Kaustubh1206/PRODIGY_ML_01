{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "OWW-I6W5JZLI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.impute import SimpleImputer, MissingIndicator\n",
        "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, Normalizer, StandardScaler, OneHotEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
        "import sklearn_pandas\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score,train_test_split\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.special import boxcox1p\n",
        "import csv\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Awcf5eWPJb9I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import scipy\n",
        "\n",
        "print('Environment specification:\\n')\n",
        "print('python', '%s.%s.%s' % sys.version_info[:3])\n",
        "\n",
        "for mod in np, scipy, sns, sklearn, pd:\n",
        "    print(mod.__name__, mod.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHIkOH1pJejm",
        "outputId": "50b50e6f-843c-404c-db30-60f0a9283a78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment specification:\n",
            "\n",
            "python 3.10.12\n",
            "numpy 1.25.2\n",
            "scipy 1.11.4\n",
            "seaborn 0.13.1\n",
            "sklearn 1.2.2\n",
            "pandas 2.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kaggle train data\n",
        "data_df = pd.read_csv('train.csv')\n",
        "# kaggle test data\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "x5MQ2t6MJhsg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "112915e9-27bc-4e8f-98d5-461a3288fd25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-891a2c502963>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# kaggle train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# kaggle test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat((data_df.loc[:,:], test_df.loc[:, :]))\n",
        "all_data.head()"
      ],
      "metadata": {
        "id": "WjFr-C9wKXXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic summary:\n",
        "data_df['SalePrice'].describe()"
      ],
      "metadata": {
        "id": "WS0Ie-yZKaC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data_df['SalePrice'])"
      ],
      "metadata": {
        "id": "9ec6f9_FKcPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Density Plot of SalePrice\n",
        "sns.distplot(data_df['SalePrice'])"
      ],
      "metadata": {
        "id": "8qz56vUPKenk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive Skeweness:\n",
        "data_df['SalePrice'].skew()"
      ],
      "metadata": {
        "id": "yjlr1pjaKhIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[\"SalePrice\"] = np.log1p(data_df[\"SalePrice\"])"
      ],
      "metadata": {
        "id": "lJKGLskpKjeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SalePrice after Log-transformation\n",
        "sns.distplot(data_df[\"SalePrice\"])\n",
        "plt.title(\"Density plot of SalePrice after Log Transformation\")"
      ],
      "metadata": {
        "id": "PYyN48Y6Kln5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = data_df[\"SalePrice\"]"
      ],
      "metadata": {
        "id": "1xPq4wi3LPut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_nan = data_df.isna().sum() / data_df.shape[0]"
      ],
      "metadata": {
        "id": "h0OFqyJZLQdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.set(font_scale=1.2)\n",
        "col_nan[col_nan > 0.01].plot(kind = \"barh\")\n",
        "plt.title(\"Features with the highest percentage of Nan values\")"
      ],
      "metadata": {
        "id": "zPAfPcDALT_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping columns for both train and test dataset\n",
        "data_df = data_df.drop(\"Id\", axis=1)\n",
        "test_df = test_df.drop(\"Id\", axis=1)"
      ],
      "metadata": {
        "id": "xdKWllg4LWtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = data_df.drop([\"Street\", \"Utilities\"], axis=1)\n",
        "test_df = test_df.drop([\"Street\", \"Utilities\"], axis=1)"
      ],
      "metadata": {
        "id": "5H0oDauDLdaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(dataset, threshold, columns=None, removed = False):\n",
        "    \"\"\"\n",
        "    Z-score method.\n",
        "    Function returns a dataframe without rows labeled as 'outliers' according to the given threshold.\n",
        "    ---------------\n",
        "    If columns = None, transform all numerical columns.\n",
        "    If removed = True, return also dataframe with removed rows.\n",
        "    \"\"\"\n",
        "    if columns==None:\n",
        "        numerics = ['int64','float64']\n",
        "        columns = dataset.select_dtypes(include=numerics).columns\n",
        "\n",
        "    tmp = dataset.copy()\n",
        "    z = np.abs(stats.zscore(tmp[columns]))\n",
        "    outliers = [row.any() for row in (z > threshold)]\n",
        "    outliers_idxs = tmp.index[outliers].tolist()\n",
        "    print(\"Number of removed rows = {}\".format(len(outliers_idxs)))\n",
        "    if removed: return dataset.drop(outliers_idxs), tmp.loc[outliers]\n",
        "    else: return dataset.drop(outliers_idxs)"
      ],
      "metadata": {
        "id": "hyPM9-6eLjYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.set(font_scale=1.2)\n",
        "sns.scatterplot(data=data_df, x=\"GrLivArea\", y=\"SalePrice\")\n",
        "plt.title(\"GrLivArea vs SalePrice\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W0_fxa-zLmXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_data = data_df.drop(data_df[(data_df['GrLivArea']>4500)].index)"
      ],
      "metadata": {
        "id": "tJkkAS_cLpJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all data together - both train and test\n",
        "train_ = clear_data.drop(['SalePrice'], axis=1)\n",
        "all_data = pd.concat([data_df, test_df]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "4bm8wEaRMZ_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Is there YearBuilt more than 2017 ? : \", all_data[all_data.YearBuilt > 2017].count()[0] != 0)\n",
        "print(\"Is there GarageYrBlt more than 2017 ? : \", all_data[all_data.GarageYrBlt > 2017].count()[0] != 0)"
      ],
      "metadata": {
        "id": "9VH2rtzPMfkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data[all_data.GarageYrBlt > 2017].GarageYrBlt"
      ],
      "metadata": {
        "id": "h3ouRch2MhdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data.loc[2590, 'GarageYrBlt'] = 2007"
      ],
      "metadata": {
        "id": "b5zgpSK4MkpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neigh_lot_frontage = all_data.groupby('Neighborhood')['LotFrontage'].agg([\"mean\", \"median\"])\n",
        "neigh_lot_frontage['avg_mean_median'] = (neigh_lot_frontage['mean'] + neigh_lot_frontage['median'] )/ 2\n",
        "neigh_lot_frontage"
      ],
      "metadata": {
        "id": "qYlCMYBvMnbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
      ],
      "metadata": {
        "id": "MrkBWzytMqLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_string(df, columns):\n",
        "    df[columns] = df[columns].astype(str)\n",
        "    return df"
      ],
      "metadata": {
        "id": "HMG7YEUvMs2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_to_categ_features = ['MSSubClass', 'OverallCond']#, 'YrSold', 'MoSold']\n",
        "\n",
        "all_data = convert_to_string(all_data, columns = num_to_categ_features)"
      ],
      "metadata": {
        "id": "yy4CEjP8Mu73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = all_data.select_dtypes(include=['int64','float64']).columns\n",
        "num_features_to_constant = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', \"MasVnrArea\"]\n",
        "num_features_to_median = [feature for feature in num_features if feature not in num_features_to_constant + [\"SalePrice\"]]"
      ],
      "metadata": {
        "id": "i1x-oSSOMxfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features_median = sklearn_pandas.gen_features(columns=[num_features_to_median],\n",
        "                                               classes=[{'class': SimpleImputer,\n",
        "                                                         'strategy': 'median',\n",
        "                                                         'missing_values' : np.nan}])\n",
        "\n",
        "numeric_features_zero = sklearn_pandas.gen_features(columns=[num_features_to_constant],\n",
        "                                               classes=[{'class': SimpleImputer,\n",
        "                                                         'strategy': 'constant',\n",
        "                                                         'fill_value' : 0,\n",
        "                                                         'missing_values' : np.nan}])\n",
        "\n",
        "missing_val_imputer = sklearn_pandas.DataFrameMapper(numeric_features_median + numeric_features_zero)\n",
        "\n",
        "# Fitting\n",
        "imputed_median = missing_val_imputer.fit(all_data)\n",
        "\n",
        "# Transformation\n",
        "imputed_features = imputed_median.transform(all_data)\n",
        "\n",
        "# Putting into dataframe\n",
        "imputed_df = pd.DataFrame(imputed_features, index=all_data.index, columns=num_features_to_median + num_features_to_constant)"
      ],
      "metadata": {
        "id": "kC3cqi13QnkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting category features\n",
        "cat_feats = all_data.select_dtypes(include=['object']).columns"
      ],
      "metadata": {
        "id": "4EINMQt3QsDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "none_conversion = [(\"MasVnrType\",\"None\"),\n",
        "                  (\"BsmtQual\",\"NA\"),\n",
        "                  (\"Electrical\", \"SBrkr\"),\n",
        "                  (\"BsmtCond\",\"TA\"),\n",
        "                  (\"BsmtExposure\",\"No\"),\n",
        "                  (\"BsmtFinType1\",\"No\"),\n",
        "                  (\"BsmtFinType2\",\"No\"),\n",
        "                  (\"CentralAir\",\"N\"),\n",
        "                  (\"Condition1\",\"Norm\"),\n",
        "                  (\"Condition2\",\"Norm\"),\n",
        "                  (\"ExterCond\",\"TA\"),\n",
        "                  (\"ExterQual\",\"TA\"),\n",
        "                  (\"FireplaceQu\",\"NA\"),\n",
        "                  (\"Functional\",\"Typ\"),\n",
        "                  (\"GarageType\",\"No\"),\n",
        "                  (\"GarageFinish\",\"No\"),\n",
        "                  (\"GarageQual\",\"NA\"),\n",
        "                  (\"GarageCond\",\"NA\"),\n",
        "                  (\"HeatingQC\",\"TA\"),\n",
        "                  (\"KitchenQual\",\"TA\"),\n",
        "                  (\"Functional\",\"Typ\"),\n",
        "                  (\"GarageType\",\"No\"),\n",
        "                  (\"GarageFinish\",\"No\"),\n",
        "                  (\"GarageQual\",\"No\"),\n",
        "                  (\"GarageCond\",\"No\"),\n",
        "                  (\"HeatingQC\",\"TA\"),\n",
        "                  (\"KitchenQual\",\"TA\"),\n",
        "                  (\"MSZoning\", \"None\"),\n",
        "                  (\"Exterior1st\", \"VinylSd\"),\n",
        "                  (\"Exterior2nd\", \"VinylSd\"),\n",
        "                  (\"SaleType\", \"WD\")]"
      ],
      "metadata": {
        "id": "SMdI6iAbQuGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def none_transform(df, conversion_list):\n",
        "    ''' Function that converts missing categorical values\n",
        "    into specific strings according to \"conversion_list\"\n",
        "\n",
        "    Returns the dataframe after transformation.\n",
        "    '''\n",
        "    for col, new_str in conversion_list:\n",
        "        df.loc[:, col] = df.loc[:, col].fillna(new_str)\n",
        "    return df"
      ],
      "metadata": {
        "id": "Hd1szonMQyZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = none_transform(all_data, none_conversion)"
      ],
      "metadata": {
        "id": "67Ji8aadQ03b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_data.columns)"
      ],
      "metadata": {
        "id": "4TaII6OkQ3dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collecting the numeric features without considering SalePrice\n",
        "numeric_features = [feat for feat in num_features if feat not in ['SalePrice']]\n",
        "\n",
        "# selecting columns with skew more than 0.5\n",
        "skewed_features = all_data[num_features].apply(lambda x: x.dropna().skew())\n",
        "skewed_features = skewed_features[skewed_features > 0.5].index\n",
        "print(\"\\nHighly skewed features: \\n\\n{}\".format(skewed_features.tolist()))"
      ],
      "metadata": {
        "id": "YuPKGcrzQ5W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_ = 0.15\n",
        "for feature in skewed_features:\n",
        "    all_data[feature] = boxcox1p(all_data[feature], lambda_)"
      ],
      "metadata": {
        "id": "QrfsLFeDQ8CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OrderedLabelTransformer(BaseEstimator, TransformerMixin):\n",
        "    orderDict = {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5}\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dict(X):\n",
        "        FirstDict = {\"Po\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4}\n",
        "        SecondDict = {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5}\n",
        "        ThirdDict = {\"NA\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4}\n",
        "        for d in [FirstDict, SecondDict, ThirdDict]:\n",
        "            if set(X) == set(d):\n",
        "                return d\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        def get_label(t):\n",
        "            return self.orderDict[t]\n",
        "        return np.array([get_label(n) for n in X])"
      ],
      "metadata": {
        "id": "O9GwJf3-Q_3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeighborhoodTransformer(BaseEstimator, TransformerMixin):\n",
        "    neighborhoodsmap = {'StoneBr' : 2, 'NridgHt' : 2, 'NoRidge': 2,\n",
        "                        'MeadowV' : 0, 'IDOTRR' : 0, 'BrDale' : 0 ,\n",
        "                        'CollgCr': 1, 'Veenker' : 1, 'Crawfor' : 1,\n",
        "                        'Mitchel' : 1, 'Somerst' : 1, 'NWAmes' : 1,\n",
        "                        'OldTown' : 1, 'BrkSide' : 1, 'Sawyer' : 1,\n",
        "                        'NAmes' : 1, 'SawyerW' : 1, 'Edwards' : 1,\n",
        "                        'Timber' : 1, 'Gilbert' : 1, 'ClearCr' : 1,\n",
        "                        'NPkVill' : 1, 'Blmngtn' : 1, 'SWISU' : 1,\n",
        "                        'Blueste': 1}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        def get_label(t):\n",
        "            return self.neighborhoodsmap[t]\n",
        "        return np.array([get_label(n) for [n] in X])"
      ],
      "metadata": {
        "id": "fnOnfP54RDR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating features:\n",
        "order_feats = [\"ExterQual\", \"ExterCond\", \"HeatingQC\", \"KitchenQual\", \"BsmtQual\",\n",
        "               \"BsmtCond\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\"]\n",
        "\n",
        "original_features_df = all_data[order_feats + ['Neighborhood']] # we need to save original values for one-hot encoding\n",
        "\n",
        "order_features = sklearn_pandas.gen_features(order_feats, [OrderedLabelTransformer])\n",
        "neighb_features = [(['Neighborhood'], [NeighborhoodTransformer()])]\n",
        "\n",
        "# Pipeline\n",
        "label_encoder = sklearn_pandas.DataFrameMapper(neighb_features + order_features)\n",
        "\n",
        "# The list with order of column names\n",
        "cols = [\"Neighborhood\"] + order_feats\n",
        "\n",
        "# Transformation both train and test set\n",
        "transformed_feats = label_encoder.fit_transform(all_data)\n",
        "\n",
        "# Putting transformed features into dataframe\n",
        "transformed_df = pd.DataFrame(transformed_feats, index=all_data.index, columns=cols)"
      ],
      "metadata": {
        "id": "P1IlA4CURHPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_features_df.shape"
      ],
      "metadata": {
        "id": "C0BbLq_XRJ0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rest_features = set(pd.concat([imputed_df, original_features_df],axis=1).columns).symmetric_difference(set(all_data.columns))\n",
        "rest_features_df = all_data[list(rest_features)]"
      ],
      "metadata": {
        "id": "9Yr17pu3RL7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([imputed_df, original_features_df, rest_features_df],axis=1)"
      ],
      "metadata": {
        "id": "BSZi-o1_RN_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data.shape"
      ],
      "metadata": {
        "id": "oeotuTZIRP8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Squere Feet for house\n",
        "all_data[\"TotalSqrtFeet\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n",
        "# test_df[\"TotalSqrtFeet\"] = test_df[\"GrLivArea\"] + test_df[\"TotalBsmtSF\"]\n",
        "\n",
        "# Total number of bathrooms\n",
        "all_data[\"TotalBaths\"] = all_data[\"BsmtFullBath\"] + (all_data[\"BsmtHalfBath\"]  * .5) + all_data[\"FullBath\"] + (all_data[\"HalfBath\"]* .5)\n",
        "# test_df[\"TotalBaths\"] = test_df[\"BsmtFullBath\"] + (test_df[\"BsmtHalfBath\"]  * .5) + test_df[\"FullBath\"] + (test_df[\"HalfBath\"]* .5)\n"
      ],
      "metadata": {
        "id": "T6-2O6EZRS5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the house has a garage\n",
        "all_data['Isgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# If the house has a fireplace\n",
        "all_data['Isfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# If the house has a pool\n",
        "all_data['Ispool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# If the house has second floor\n",
        "all_data['Issecondfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# If the house has Open Porch\n",
        "all_data['IsOpenPorch'] = all_data['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# If the house has Wood Deck\n",
        "all_data['IsWoodDeck'] = all_data['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)"
      ],
      "metadata": {
        "id": "iQ9xS4GBtmyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = all_data.drop([\"SalePrice\"], axis = 1)\n",
        "\n",
        "hot_one_features = pd.get_dummies(all_data).reset_index(drop=True)\n",
        "hot_one_features.shape"
      ],
      "metadata": {
        "id": "paNqTEMjtsBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([transformed_df, hot_one_features],axis=1)"
      ],
      "metadata": {
        "id": "-m1QYlNatskl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preprocessed = all_data.iloc[:len(data_df),:]\n",
        "test_preprocessed = all_data.iloc[len(train_preprocessed):,:]\n",
        "print(len(test_preprocessed) == len(test_df))"
      ],
      "metadata": {
        "id": "vBxCKFbgtvDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_preprocessed"
      ],
      "metadata": {
        "id": "LAgRwuL2txa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.linear_model import ElasticNet, Lasso, ElasticNetCV\n",
        "from sklearn.ensemble import  GradientBoostingRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.base import RegressorMixin\n",
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "Jt5jCGAht0Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Environment specification:\\n')\n",
        "for mod in sklearn, xgb, lgb:\n",
        "    print(mod.__name__, mod.__version__)"
      ],
      "metadata": {
        "id": "XPXhk70Jt2u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(model):\n",
        "    n_folds=5\n",
        "    kfold = KFold(n_folds, random_state=42, shuffle=True).get_n_splits(X_train)\n",
        "    rmse_score = np.sqrt(-cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = kfold, verbose = -1, n_jobs=-1))\n",
        "    return(np.mean(rmse_score))"
      ],
      "metadata": {
        "id": "1jYiQSrmt-ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = make_pipeline(RobustScaler(), LinearRegression()) #TODO: why Robust Scaler?\n",
        "\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_train_pred = lr_model.predict(X_train)\n",
        "MSE_train = np.mean((y_train_pred - y_train)**2)\n",
        "\n",
        "print(\"Mean Squared Error = {:.8f}\".format(MSE_train))\n",
        "print(\"RMSE score for Linear Regression: {:.3f}\".format(rmse(lr_model)))"
      ],
      "metadata": {
        "id": "ek7Tyw1JuDHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale=1.5)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=y_train, y=y_train_pred)  # Specify x and y separately\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"Actual Prices vs. Predicted Prices\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uFauA53iuFqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate residuals\n",
        "residuals = y_train_pred - y_train\n",
        "\n",
        "# Residual plot - result should be randomly located around the 0 value\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=y_train_pred, y=residuals)\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.xlabel(\"Predicted values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.axhline(0, color='red', linestyle='--')  # Add a horizontal line at y=0 for reference\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s26Yx4mIuIVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_model = make_pipeline(RobustScaler(),\n",
        "                         LassoCV(alphas = [0.0004, 0.0005, 0.0006],\n",
        "                                 random_state = 0,\n",
        "                                 cv = 10))\n",
        "\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = lasso_model.predict(X_train)\n",
        "MSE_train = np.mean((y_train_pred - y_train)**2)\n",
        "\n",
        "# print(\"Best alpha : {}\", lasso_model.alpha_)\n",
        "print(\"Mean Squared Error = {:.8f}\".format(MSE_train))\n",
        "print(\"RMSE score for LASSO: {:.3f}\".format(rmse(lasso_model)))"
      ],
      "metadata": {
        "id": "E9kqnpYnv3GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting predictions\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=y_train_pred, y=y_train)  # Specify x and y separately\n",
        "plt.title(\"Linear regression with Lasso regularization\")\n",
        "plt.xlabel(\"Predicted Prices\")\n",
        "plt.ylabel(\"Real Prices\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iPhFppfwv5sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbr = GradientBoostingRegressor(random_state=0)\n",
        "param_grid = {'n_estimators': [2500],\n",
        "              'max_features': [13],\n",
        "              'max_depth': [5],\n",
        "              'learning_rate': [0.05],\n",
        "              'subsample': [0.8],\n",
        "             'random_state' : [5]}\n",
        "\n",
        "gb_model = GridSearchCV(estimator=gbr, param_grid=param_grid, n_jobs=1, cv=5)\n",
        "gb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-Ft_9f5DwKR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = gb_model.predict(X_train)\n",
        "MSE_train = np.mean((y_train_pred - y_train)**2)\n",
        "print('Best Parameters: {}'.format(gb_model.best_params_))\n",
        "print(\"Mean Squared Error = {:.8f}\".format(MSE_train))\n",
        "print(\"RMSE score for GB: {:.3f}\".format(rmse(gb_model)))"
      ],
      "metadata": {
        "id": "w88As9LawTx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Define your XGBRegressor\n",
        "xgbreg = xgb.XGBRegressor(seed=0)\n",
        "\n",
        "# Define a reduced search space\n",
        "param_dist = {'n_estimators': [2000],\n",
        "              'learning_rate': [0.05],\n",
        "              'max_depth': [3, 7],\n",
        "              'subsample': [0.8],\n",
        "              'colsample_bytree': [0.45, 0.75]}\n",
        "\n",
        "# Use RandomizedSearchCV for faster parameter tuning\n",
        "xgb_model = RandomizedSearchCV(estimator=xgbreg, param_distributions=param_dist, n_iter=10, n_jobs=-1, cv=10, random_state=0)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions on the training set\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "MSE_train = np.mean((y_train_pred - y_train) ** 2)\n",
        "\n",
        "print('\\n\\nBest Parameters: {}'.format(xgb_model.best_params_))\n",
        "print(\"Mean Squared Error = {:.8f}\".format(MSE_train))\n",
        "print(\"RMSE score for XGB: {:.3f}\".format(np.sqrt(MSE_train)))\n"
      ],
      "metadata": {
        "id": "RsrMHoTLwWcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_model = ElasticNetCV(alphas = [0.0001, 0.0003, 0.0004, 0.0006],\n",
        "                        l1_ratio = [.9, .92],\n",
        "                        random_state = 0,\n",
        "                        cv=10)"
      ],
      "metadata": {
        "id": "S4_ENgWS3Flk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = en_model.predict(X_train)\n",
        "MSE_train = np.mean((y_train_pred - y_train)**2)"
      ],
      "metadata": {
        "id": "XbUt9-5S3Iar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean Squared Error = {:.8f}\".format(MSE_train))\n",
        "print(\"RMSE score for ElasticNet: {:.3f}\".format(rmse(en_model)))"
      ],
      "metadata": {
        "id": "__Pv_cHn3MU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_model = lgb.LGBMRegressor(objective='regression', num_leaves=5,\n",
        "                              learning_rate=0.05, n_estimators=800,\n",
        "                              max_bin = 55, bagging_fraction = 0.8,\n",
        "                              bagging_freq = 5, feature_fraction = 0.2,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
        "\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = lgb_model.predict(X_train)\n",
        "MSE_train = np.mean((y_train_pred - y_train)**2)\n",
        "\n",
        "print(\"Mean Squared Error = {:.8f}\".format(MSE_train))\n",
        "print(\"RMSE score for LGBMRegressor: {:.4f}\".format(rmse(lgb_model)))"
      ],
      "metadata": {
        "id": "HpB1V5WS3O4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "model2 = sklearn.ensemble.BaggingRegressor(\n",
        "    base_estimator=en_model,\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,  # Adjust max_samples (e.g., reduce it to 0.8)\n",
        "    max_features=200,\n",
        "    verbose=3,\n",
        "    n_jobs=3\n",
        ")\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = model2.predict(X_train)\n",
        "MSE_train = np.mean((y_train_pred - y_train)**2)\n",
        "\n",
        "print(\"Mean Squared Error = {:.8f}\".format(MSE_train))"
      ],
      "metadata": {
        "id": "sloAIQE_7q5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RMSE score for BaggingRegressor: {:.4f}\".format(rmse(model2)))"
      ],
      "metadata": {
        "id": "o6Zs2Ad18YgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.regressor import StackingCVRegressor\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "metadata": {
        "id": "dNF4lCWo8bUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_model = make_pipeline(RobustScaler(),\n",
        "                      LassoCV(max_iter=1e7, alphas = [0.0005],\n",
        "                              random_state = 42, cv=5))\n",
        "\n",
        "elasticnet_model = make_pipeline(RobustScaler(),\n",
        "                           ElasticNetCV(max_iter=1e7, alphas=[0.0005],\n",
        "                                        cv=5, l1_ratio=0.9))\n",
        "\n",
        "lgbm_model = make_pipeline(RobustScaler(),\n",
        "                        lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
        "                                      learning_rate=0.05, n_estimators=800,\n",
        "                                      max_bin = 55, bagging_fraction = 0.8,\n",
        "                                      bagging_freq = 5, feature_fraction = 0.23,\n",
        "                                      feature_fraction_seed = 9, bagging_seed=9,\n",
        "                                      min_data_in_leaf = 6,\n",
        "                                      min_sum_hessian_in_leaf = 11))\n",
        "\n",
        "xgboost_model = make_pipeline(RobustScaler(),\n",
        "                        xgb.XGBRegressor(learning_rate = 0.01, n_estimators=3400,\n",
        "                                     max_depth=3,min_child_weight=0 ,\n",
        "                                     gamma=0, subsample=0.7,\n",
        "                                     colsample_bytree=0.7,\n",
        "                                     objective= 'reg:linear',nthread=4,\n",
        "                                     scale_pos_weight=1,seed=27,\n",
        "                                     reg_alpha=0.00006))"
      ],
      "metadata": {
        "id": "gnKaPrwa8djt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_regressor = StackingCVRegressor(regressors=(lasso_model, elasticnet_model, xgboost_model, lgbm_model),\n",
        "                               meta_regressor=xgboost_model, use_features_in_secondary=True)"
      ],
      "metadata": {
        "id": "Emivh7PZI4AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_regressor = LassoCV(max_iter=10000000)  # Use an integer value for max_iter\n",
        "stack_model = stack_regressor.fit(np.array(X_train), np.array(y_train))\n"
      ],
      "metadata": {
        "id": "CF6eI0CZI47T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_preds = en_model.predict(test_preprocessed)"
      ],
      "metadata": {
        "id": "jwxuX9NOI6pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Create a RobustScaler instance\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit the scaler on your training data\n",
        "scaler.fit(X_train)  # Assuming X_train is your training data\n",
        "\n",
        "# Transform your test data using the fitted scaler\n",
        "test_preprocessed = scaler.transform(test_preprocessed)  # Assuming test_preprocessed is your test data\n",
        "\n",
        "# Create a Lasso model\n",
        "lasso_model = Lasso(alpha=1.0)  # You can adjust the alpha parameter as needed\n",
        "\n",
        "# Fit the Lasso model on your training data\n",
        "lasso_model.fit(X_train, y_train)  # Assuming X_train and y_train are your training data and labels\n",
        "\n",
        "# Now, you can use the Lasso model to make predictions\n",
        "lasso_preds = lasso_model.predict(test_preprocessed)\n"
      ],
      "metadata": {
        "id": "cX1cn1KUKPs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_gen_preds = stack_model.predict(test_preprocessed)"
      ],
      "metadata": {
        "id": "S4dzrzh1KSFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_preds = lgb_model.predict(test_preprocessed)"
      ],
      "metadata": {
        "id": "nXSZuS8bKsxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted predictions\n",
        "stack_preds = ((0.2*en_preds) + (0.25*lasso_preds) + (0.15*lgbm_preds) + (0.4*stack_gen_preds))"
      ],
      "metadata": {
        "id": "_CgUwtHTKu5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming stack_preds is your predicted values\n",
        "predictions_df = pd.DataFrame(np.expm1(stack_preds),\n",
        "                              columns=[\"SalePrice\"])\n",
        "\n",
        "# Create an index starting from 1\n",
        "predictions_df.index = predictions_df.index + 1\n",
        "\n",
        "# Set the index name to \"Id\"\n",
        "predictions_df.index.name = \"Id\"\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "predictions_df.head()\n"
      ],
      "metadata": {
        "id": "81jWFt18KxFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df[\"SalePrice\"].to_csv(\"my_predictions.csv\", header=True)"
      ],
      "metadata": {
        "id": "0-Lzuxc9KzAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}